{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216a2476-f789-49b7-a7f6-261617d6e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, to_json, lit, collect_list, size, avg\n",
    "\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064ae0b6-844f-4997-9a3d-d910e5aba575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configura os logs\n",
    "LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n",
    "\n",
    "def setup_logger():\n",
    "    \"\"\"\n",
    "    Configura o logger para o formato desejado.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        level=logging.INFO,\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235e8293-1513-4c17-bc97-6ca9857b4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configura sessao spark\n",
    "def sessao_spark(app_name):\n",
    "\n",
    "    minio_endpoint = \"http://minio:9000\"\n",
    "    minio_access_key = \"lakehouse\"\n",
    "    minio_secret_key = \"2fUDaiyqNFhoMmgYuXjO4d24fchviXQjM2TWTgUe\"\n",
    "\n",
    "    # Lista todos os JARs na pasta\n",
    "    jars_dir = \"./jars\"\n",
    "    jars = [os.path.join(jars_dir, jar) for jar in os.listdir(jars_dir) if jar.endswith(\".jar\")]\n",
    "    jars_str = \",\".join(jars)\n",
    "    \n",
    "    # Configuração do Spark\n",
    "    spark = (\n",
    "        SparkSession \n",
    "        .builder \n",
    "        .appName(f\"{app_name}\") \n",
    "        .master(\"spark://spark-master:7077\") \n",
    "        .config(\"spark.jars\", jars_str)\n",
    "        .config(\"spark.executor.memory\", \"3g\") \n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint) \n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", minio_access_key) \n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", minio_secret_key) \n",
    "        .config(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d8a000-e9af-4285-9c9f-bf43a9115fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 18:21:55 - __main__ - INFO - Iniciando o script.\n",
      "2025-05-11 18:21:55 - __main__ - INFO - <pyspark.sql.session.SparkSession object at 0x747ad637bad0>\n",
      "2025-05-11 18:21:55 - __main__ - INFO - Configs: [('spark.hadoop.fs.s3a.multipart.size', '104857600'), ('spark.executor.memory', '3g'), ('spark.hadoop.fs.s3a.path.style.access', 'true'), ('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider'), ('spark.jars', 'file:///home/jovyan/notebooks/jars/hive-exec-3.1.2.jar,file:///home/jovyan/notebooks/jars/delta-spark_2.12-3.2.0.jar,file:///home/jovyan/notebooks/jars/spark-measure_2.12-0.24.jar,file:///home/jovyan/notebooks/jars/hive-metastore-3.1.2.jar,file:///home/jovyan/notebooks/jars/hadoop-aws-3.3.4.jar,file:///home/jovyan/notebooks/jars/delta-storage-3.2.0.jar,file:///home/jovyan/notebooks/jars/postgresql-42.5.1.jar,file:///home/jovyan/notebooks/jars/aws-java-sdk-bundle-1.11.1026.jar,file:///home/jovyan/notebooks/jars/antlr4-runtime-4.9.3.jar,file:///home/jovyan/notebooks/jars/libthrift-0.12.0.jar,file:///home/jovyan/notebooks/jars/libfb303-0.9.3.jar'), ('spark.repl.local.jars', 'file:///home/jovyan/notebooks/jars/hive-exec-3.1.2.jar,file:///home/jovyan/notebooks/jars/delta-spark_2.12-3.2.0.jar,file:///home/jovyan/notebooks/jars/spark-measure_2.12-0.24.jar,file:///home/jovyan/notebooks/jars/hive-metastore-3.1.2.jar,file:///home/jovyan/notebooks/jars/hadoop-aws-3.3.4.jar,file:///home/jovyan/notebooks/jars/delta-storage-3.2.0.jar,file:///home/jovyan/notebooks/jars/postgresql-42.5.1.jar,file:///home/jovyan/notebooks/jars/aws-java-sdk-bundle-1.11.1026.jar,file:///home/jovyan/notebooks/jars/antlr4-runtime-4.9.3.jar,file:///home/jovyan/notebooks/jars/libthrift-0.12.0.jar,file:///home/jovyan/notebooks/jars/libfb303-0.9.3.jar'), ('spark.executor.cores', '2'), ('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'), ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'), ('spark.master', 'spark://spark-master:7077'), ('spark.hadoop.fs.s3a.secret.key', '2fUDaiyqNFhoMmgYuXjO4d24fchviXQjM2TWTgUe'), ('spark.hadoop.fs.s3a.connection.maximum', '100'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000'), ('spark.app.submitTime', '1746987713433'), ('spark.delta.logStore.class', 'org.apache.spark.sql.delta.storage.S3SingleDriverLogStore'), ('spark.hadoop.fs.s3a.access.key', 'lakehouse'), ('spark.sql.files.maxPartitionBytes', '134217728'), ('spark.app.name', 'delta-time-travel'), ('spark.hadoop.fs.s3a.fast.upload', 'true'), ('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "# configs\n",
    "spark = sessao_spark('delta-time-travel')\n",
    "\n",
    "logger = setup_logger()\n",
    "logger.info(\"Iniciando o script.\")\n",
    "\n",
    "logger.info(spark)\n",
    "logger.info(f\"Configs: {SparkConf().getAll()}\")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "minio_bucket = f\"s3a://production/landing/dsa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2845ba5-fc2b-4675-9c01-1e42c548ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos dados\n",
    "dsa_dados = spark.read.csv(\n",
    "    f\"{minio_bucket}/csv/dados_iniciais.csv\",\n",
    "    header = True,\n",
    "    inferSchema = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9577c4a-e5d3-4f8c-945b-48b5c46a9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravando os dados em formato delta\n",
    "(\n",
    "    dsa_dados\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(f\"{minio_bucket}/lakehouse/dados_iniciais\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75460c-5348-491a-a481-e5e17282c016",
   "metadata": {},
   "source": [
    "## Manipulando os Dados no Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c990b3d-9410-4272-919a-56950cc3022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 18:23:09 - __main__ - INFO - Dados iniciais:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-------------------+\n",
      "| id|    nome|idade|             funcao|\n",
      "+---+--------+-----+-------------------+\n",
      "|  1|   Lucas|   30| Cientista de Dados|\n",
      "|  2|   Bruno|   18|  Analista de Dados|\n",
      "|  3| Mariana|   35| Arquiteto de Dados|\n",
      "|  4|Fernando|   40|Engenheiro de Dados|\n",
      "|  5| Gabriel|   28|   Engenheiro de IA|\n",
      "|  6|  Camila|   50|   Engenheiro de ML|\n",
      "|  7|  Amanda|   29| Engenheiro DataOps|\n",
      "|  8| Juliano|   43| Arquiteto de Dados|\n",
      "|  9| Gustavo|   56| Arquiteto de Dados|\n",
      "| 10| Vanessa|   31|  Analista de Dados|\n",
      "+---+--------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Dados iniciais:\")\n",
    "delta_table_path = f\"{minio_bucket}/lakehouse/dados_iniciais\"\n",
    "spark.read.format(\"delta\").load(delta_table_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120942ff-68a5-4533-bd06-6feed62d5510",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f0757c5-0ff7-445c-8720-63b64ce84c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após atualização de Lucas:\n",
      "+---+--------+-----+-----------------------+\n",
      "|id |nome    |idade|funcao                 |\n",
      "+---+--------+-----+-----------------------+\n",
      "|1  |Lucas   |32   |Gerente de Data Science|\n",
      "|2  |Bruno   |18   |Analista de Dados      |\n",
      "|3  |Mariana |35   |Arquiteto de Dados     |\n",
      "|4  |Fernando|40   |Engenheiro de Dados    |\n",
      "|5  |Gabriel |28   |Engenheiro de IA       |\n",
      "|6  |Camila  |50   |Engenheiro de ML       |\n",
      "|7  |Amanda  |29   |Engenheiro DataOps     |\n",
      "|8  |Juliano |43   |Arquiteto de Dados     |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados     |\n",
      "|10 |Vanessa |31   |Analista de Dados      |\n",
      "+---+--------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table.update(\n",
    "    condition = \"nome = 'Lucas'\",\n",
    "    set = {\"idade\": \"32\", \"funcao\": \"'Gerente de Data Science'\"}\n",
    ")\n",
    "print(\"Após atualização de Lucas:\")\n",
    "delta_table.toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662091a7-b130-49b6-8f08-e937b35be453",
   "metadata": {},
   "source": [
    "### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192fb9c6-9834-41b3-8008-a38cf14a2c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após remoção de funcionários com idade <= 30:\n",
      "+---+--------+-----+-----------------------+\n",
      "|id |nome    |idade|funcao                 |\n",
      "+---+--------+-----+-----------------------+\n",
      "|1  |Lucas   |32   |Gerente de Data Science|\n",
      "|3  |Mariana |35   |Arquiteto de Dados     |\n",
      "|4  |Fernando|40   |Engenheiro de Dados    |\n",
      "|6  |Camila  |50   |Engenheiro de ML       |\n",
      "|8  |Juliano |43   |Arquiteto de Dados     |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados     |\n",
      "|10 |Vanessa |31   |Analista de Dados      |\n",
      "+---+--------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.delete(condition = \"idade <= 30\")\n",
    "print(\"Após remoção de funcionários com idade <= 30:\")\n",
    "delta_table.toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a479d9f-eb7b-461a-b2a5-cdf4a6c82732",
   "metadata": {},
   "source": [
    "### Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1060dd08-6f89-45b0-adc2-82e2b492953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 18:25:08 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após inserção de novos registros:\n",
      "+---+--------+-----+-----------------------+\n",
      "|id |nome    |idade|funcao                 |\n",
      "+---+--------+-----+-----------------------+\n",
      "|1  |Lucas   |32   |Gerente de Data Science|\n",
      "|3  |Mariana |35   |Arquiteto de Dados     |\n",
      "|4  |Fernando|40   |Engenheiro de Dados    |\n",
      "|6  |Camila  |50   |Engenheiro de ML       |\n",
      "|8  |Juliano |43   |Arquiteto de Dados     |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados     |\n",
      "|10 |Vanessa |31   |Analista de Dados      |\n",
      "|11 |Leonardo|27   |Analytics Engineer     |\n",
      "|14 |Melissa |26   |Cientista de Dados     |\n",
      "|12 |Felipe  |31   |Analytics Engineer     |\n",
      "|13 |Paula   |26   |Engenheiro de Dados    |\n",
      "+---+--------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_employees = spark.createDataFrame([\n",
    "    (11, \"Leonardo\", 27, \"Analytics Engineer\"),\n",
    "    (12, \"Felipe\", 31, \"Analytics Engineer\"),\n",
    "    (13, \"Paula\", 26, \"Engenheiro de Dados\"),\n",
    "    (14, \"Melissa\", 26, \"Cientista de Dados\")\n",
    "], [\"id\", \"nome\", \"idade\", \"funcao\"])\n",
    "\n",
    "delta_table.alias(\"existingData\").merge(\n",
    "    new_employees.alias(\"newData\"),\n",
    "    \"existingData.id = newData.id\"\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "print(\"Após inserção de novos registros:\")\n",
    "delta_table.toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c319d03-213b-4410-9807-dc818f5a582a",
   "metadata": {},
   "source": [
    "### Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c09c99-d378-4aa1-a146-9d099ef47e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após upsert (atualização/inserção):\n",
      "+---+--------+-----+-----------------------+\n",
      "|id |nome    |idade|funcao                 |\n",
      "+---+--------+-----+-----------------------+\n",
      "|1  |Lucas   |32   |Gerente de Data Science|\n",
      "|3  |Mariana |36   |Arquiteto de Dados     |\n",
      "|4  |Fernando|40   |Engenheiro de Dados    |\n",
      "|6  |Camila  |50   |Engenheiro de ML       |\n",
      "|8  |Juliano |43   |Arquiteto de Dados     |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados     |\n",
      "|10 |Vanessa |31   |Analista de Dados      |\n",
      "|15 |Tales   |24   |Arquiteto RPA          |\n",
      "|11 |Leonardo|27   |Analytics Engineer     |\n",
      "|14 |Melissa |26   |Cientista de Dados     |\n",
      "|12 |Felipe  |31   |Analytics Engineer     |\n",
      "|13 |Paula   |26   |Engenheiro de Dados    |\n",
      "+---+--------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upsert_data = spark.createDataFrame([\n",
    "    (3, \"Mariana\", 36, \"Arquiteto de Dados\"),  # Atualizar idade da Mariana\n",
    "    (15, \"Tales\", 24, \"Arquiteto RPA\")         # Novo registro\n",
    "], [\"id\", \"nome\", \"idade\", \"funcao\"])\n",
    "\n",
    "delta_table.alias(\"oldData\").merge(\n",
    "    upsert_data.alias(\"upsertData\"),\n",
    "    \"oldData.id = upsertData.id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"idade\": \"upsertData.idade\",\n",
    "    \"funcao\": \"upsertData.funcao\"\n",
    "}).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "print(\"Após upsert (atualização/inserção):\")\n",
    "delta_table.toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d866bbad-5407-41dd-8902-c282fab5d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histórico de Alterações (Time Travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8891d7-3159-4b08-8da2-f16e4114391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tabela tem 5 versões.\n"
     ]
    }
   ],
   "source": [
    "# Caminho para a tabela Delta\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Obter o histórico completo\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# Contar o número de versões\n",
    "num_versions = history_df.count()\n",
    "print(f\"A tabela tem {num_versions} versões.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51dfecdb-fd0a-4af1-bed1-4c25aed78a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão inicial da tabela:\n",
      "+---+--------+-----+-------------------+\n",
      "|id |nome    |idade|funcao             |\n",
      "+---+--------+-----+-------------------+\n",
      "|1  |Lucas   |30   |Cientista de Dados |\n",
      "|2  |Bruno   |18   |Analista de Dados  |\n",
      "|3  |Mariana |35   |Arquiteto de Dados |\n",
      "|4  |Fernando|40   |Engenheiro de Dados|\n",
      "|5  |Gabriel |28   |Engenheiro de IA   |\n",
      "|6  |Camila  |50   |Engenheiro de ML   |\n",
      "|7  |Amanda  |29   |Engenheiro DataOps |\n",
      "|8  |Juliano |43   |Arquiteto de Dados |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados |\n",
      "|10 |Vanessa |31   |Analista de Dados  |\n",
      "+---+--------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Acessar a versão mais antiga da tabela (versão 0)\n",
    "print(\"Versão inicial da tabela:\")\n",
    "old_version = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "old_version.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a5cff11-29e7-468e-b244-235c38052980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão 0 da tabela:\n",
      "+---+--------+-----+-------------------+\n",
      "|id |nome    |idade|funcao             |\n",
      "+---+--------+-----+-------------------+\n",
      "|1  |Lucas   |30   |Cientista de Dados |\n",
      "|2  |Bruno   |18   |Analista de Dados  |\n",
      "|3  |Mariana |35   |Arquiteto de Dados |\n",
      "|4  |Fernando|40   |Engenheiro de Dados|\n",
      "|5  |Gabriel |28   |Engenheiro de IA   |\n",
      "|6  |Camila  |50   |Engenheiro de ML   |\n",
      "|7  |Amanda  |29   |Engenheiro DataOps |\n",
      "|8  |Juliano |43   |Arquiteto de Dados |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados |\n",
      "|10 |Vanessa |31   |Analista de Dados  |\n",
      "+---+--------+-----+-------------------+\n",
      "\n",
      "Versão 1 da tabela:\n",
      "+---+--------+-----+-----------------------+\n",
      "|id |nome    |idade|funcao                 |\n",
      "+---+--------+-----+-----------------------+\n",
      "|1  |Lucas   |32   |Gerente de Data Science|\n",
      "|2  |Bruno   |18   |Analista de Dados      |\n",
      "|3  |Mariana |35   |Arquiteto de Dados     |\n",
      "|4  |Fernando|40   |Engenheiro de Dados    |\n",
      "|5  |Gabriel |28   |Engenheiro de IA       |\n",
      "|6  |Camila  |50   |Engenheiro de ML       |\n",
      "|7  |Amanda  |29   |Engenheiro DataOps     |\n",
      "|8  |Juliano |43   |Arquiteto de Dados     |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados     |\n",
      "|10 |Vanessa |31   |Analista de Dados      |\n",
      "+---+--------+-----+-----------------------+\n",
      "\n",
      "Versão 4 da tabela:\n",
      "+---+--------+-----+-----------------------+\n",
      "|id |nome    |idade|funcao                 |\n",
      "+---+--------+-----+-----------------------+\n",
      "|1  |Lucas   |32   |Gerente de Data Science|\n",
      "|3  |Mariana |36   |Arquiteto de Dados     |\n",
      "|4  |Fernando|40   |Engenheiro de Dados    |\n",
      "|6  |Camila  |50   |Engenheiro de ML       |\n",
      "|8  |Juliano |43   |Arquiteto de Dados     |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados     |\n",
      "|10 |Vanessa |31   |Analista de Dados      |\n",
      "|15 |Tales   |24   |Arquiteto RPA          |\n",
      "|11 |Leonardo|27   |Analytics Engineer     |\n",
      "|14 |Melissa |26   |Cientista de Dados     |\n",
      "|12 |Felipe  |31   |Analytics Engineer     |\n",
      "|13 |Paula   |26   |Engenheiro de Dados    |\n",
      "+---+--------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Versão 0 da tabela:\")\n",
    "version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "version_0.show(truncate=False)\n",
    "\n",
    "print(\"Versão 1 da tabela:\")\n",
    "version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path)\n",
    "version_1.show(truncate=False)\n",
    "\n",
    "print(\"Versão 4 da tabela:\")\n",
    "version_4 = spark.read.format(\"delta\").option(\"versionAsOf\", 4).load(delta_table_path)\n",
    "version_4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f3b521-4515-42d9-b56f-65cbe5a23df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar uma coluna que identifica a versão\n",
    "version_0 = version_0.withColumn(\"versao\", lit(0))\n",
    "version_4 = version_4.withColumn(\"versao\", lit(4))\n",
    "\n",
    "# Unir as duas versões\n",
    "changes = version_0.union(version_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6859ed88-aa87-46b5-b9f5-dee80efb559c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alterações entre versões:\n",
      "+---+--------+-------+\n",
      "|id |nome    |versoes|\n",
      "+---+--------+-------+\n",
      "|15 |Tales   |[4]    |\n",
      "|12 |Felipe  |[4]    |\n",
      "|11 |Leonardo|[4]    |\n",
      "|13 |Paula   |[4]    |\n",
      "|7  |Amanda  |[0]    |\n",
      "|5  |Gabriel |[0]    |\n",
      "|2  |Bruno   |[0]    |\n",
      "|14 |Melissa |[4]    |\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar as diferenças em relação ao nome\n",
    "print(\"Alterações entre versões:\")\n",
    "changes.groupBy(\"id\", \"nome\") \\\n",
    "       .agg(collect_list(\"versao\").alias(\"versoes\")) \\\n",
    "       .filter(size(\"versoes\") == 1) \\\n",
    "       .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e71ccd4-360d-48b3-a4ca-e143b848a1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alterações entre versões:\n",
      "+---+-----+-------+\n",
      "|id |idade|versoes|\n",
      "+---+-----+-------+\n",
      "|15 |24   |[4]    |\n",
      "|1  |32   |[4]    |\n",
      "|3  |36   |[4]    |\n",
      "|12 |31   |[4]    |\n",
      "|11 |27   |[4]    |\n",
      "|13 |26   |[4]    |\n",
      "|5  |28   |[0]    |\n",
      "|3  |35   |[0]    |\n",
      "|2  |18   |[0]    |\n",
      "|1  |30   |[0]    |\n",
      "|7  |29   |[0]    |\n",
      "|14 |26   |[4]    |\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar as diferenças em relação a idade\n",
    "print(\"Alterações entre versões:\")\n",
    "changes.groupBy(\"id\", \"idade\") \\\n",
    "       .agg(collect_list(\"versao\").alias(\"versoes\")) \\\n",
    "       .filter(size(\"versoes\") == 1) \\\n",
    "       .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ac84313-cc03-4bd2-81d6-5e397c28a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferença de média de idade entre versões:\n",
      "+------+------------------+\n",
      "|versao|       media_idade|\n",
      "+------+------------------+\n",
      "|     4|35.166666666666664|\n",
      "|     0|              36.0|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular a média de idade por versão\n",
    "avg_ages = version_0.union(version_4) \\\n",
    "    .groupBy(\"versao\") \\\n",
    "    .agg(avg(\"idade\").alias(\"media_idade\"))\n",
    "\n",
    "# Mostrar a diferença de média de idade entre as versões\n",
    "print(\"Diferença de média de idade entre versões:\")\n",
    "avg_ages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f59efe4f-3ae8-41a4-bb21-2678c8e704bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histórico de alterações da tabela Delta (formatado):\n",
      "+------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "|Versão|Operação|Métricas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |Metadados do Usuário|\n",
      "+------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "|4     |MERGE   |{numTargetRowsCopied -> 6, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1480, numTargetBytesRemoved -> 1444, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 2332, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1384, numTargetRowsUpdated -> 1, numOutputRows -> 8, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 670}|NULL                |\n",
      "|3     |MERGE   |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 4, numTargetBytesAdded -> 5079, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 2804, numTargetRowsInserted -> 4, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 4, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 2786}     |NULL                |\n",
      "|2     |DELETE  |{numRemovedFiles -> 1, numRemovedBytes -> 1515, numCopiedRows -> 7, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1157, numDeletionVectorsUpdated -> 0, numDeletedRows -> 3, scanTimeMs -> 904, numAddedFiles -> 1, numAddedBytes -> 1444, rewriteTimeMs -> 253}                                                                                                                                                                                                                                                                                                                          |NULL                |\n",
      "|1     |UPDATE  |{numRemovedFiles -> 1, numRemovedBytes -> 1489, numCopiedRows -> 9, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 2915, numDeletionVectorsUpdated -> 0, scanTimeMs -> 1610, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1515, rewriteTimeMs -> 1304}                                                                                                                                                                                                                                                                                                                        |NULL                |\n",
      "|0     |WRITE   |{numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 1489}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |NULL                |\n",
      "+------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carrega a tabela delta\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Carregar o histórico de alterações da tabela Delta\n",
    "history = delta_table.history()\n",
    "\n",
    "# Selecionar apenas as colunas relevantes\n",
    "formatted_history = history.select(\n",
    "    col(\"version\").alias(\"Versão\"),\n",
    "    col(\"operation\").alias(\"Operação\"),\n",
    "    col(\"operationMetrics\").alias(\"Métricas\"),\n",
    "    col(\"userMetadata\").alias(\"Metadados do Usuário\")\n",
    ")\n",
    "\n",
    "# Mostrar as alterações \n",
    "print(\"Histórico de alterações da tabela Delta (formatado):\")\n",
    "formatted_history.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da300c30-4cf7-4439-80bf-e3b517ad1e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "|Versão|Operação|Métricas                                                                                                                                                                                                                                                                                                                       |Metadados do Usuário|\n",
      "+------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "|1     |UPDATE  |{numRemovedFiles -> 1, numRemovedBytes -> 1489, numCopiedRows -> 9, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 2915, numDeletionVectorsUpdated -> 0, scanTimeMs -> 1610, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1515, rewriteTimeMs -> 1304}|NULL                |\n",
      "+------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se você quiser exibir apenas operações específicas (por exemplo, UPDATE), pode usar .filter():\n",
    "filtered_history = formatted_history.filter(col(\"Operação\") == \"UPDATE\")\n",
    "filtered_history.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "973daf51-49cb-4be5-b9cb-d254033b6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histórico salvo em: s3a://production/landing/dsa/csv/output\n"
     ]
    }
   ],
   "source": [
    "# Carregar o histórico de alterações da tabela Delta\n",
    "history = delta_table.history()\n",
    "\n",
    "# Selecionar e formatar as colunas\n",
    "formatted_history = history.select(\n",
    "    col(\"version\").alias(\"Versão\"),\n",
    "    col(\"operation\").alias(\"Operação\"),\n",
    "    to_json(col(\"operationMetrics\")).alias(\"Métricas\"),  # Converter MAP para JSON, para conseguir salvar em CSV\n",
    "    col(\"userMetadata\").alias(\"Metadados do Usuário\")\n",
    ")\n",
    "\n",
    "# Salvar o histórico formatado em CSV\n",
    "output_path = f\"{minio_bucket}/csv/output\"\n",
    "formatted_history.write.format(\"csv\").option(\"header\", \"true\").save(output_path)\n",
    "\n",
    "print(f\"Histórico salvo em: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e070eeb-a2ea-4053-81fe-d36f40ab08e5",
   "metadata": {},
   "source": [
    "Embora não haja um comando direto de rollback no Delta Lake, você pode sobrescrever uma nova versão com os dados de uma versão anterior \n",
    "sem perder todo o histórico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f1030a-8ff5-473e-976f-030d92218ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-----------------------+\n",
      "|id |nome    |idade|funcao                 |\n",
      "+---+--------+-----+-----------------------+\n",
      "|1  |Lucas   |32   |Gerente de Data Science|\n",
      "|3  |Mariana |35   |Arquiteto de Dados     |\n",
      "|4  |Fernando|40   |Engenheiro de Dados    |\n",
      "|6  |Camila  |50   |Engenheiro de ML       |\n",
      "|8  |Juliano |43   |Arquiteto de Dados     |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados     |\n",
      "|10 |Vanessa |31   |Analista de Dados      |\n",
      "+---+--------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultar uma versão antiga (versão 2)\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(delta_table_path).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f220caa6-e857-4fc7-9fa1-ef53d3f89eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar a versão 2\n",
    "old_version = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4468737a-c428-4095-bd1c-1a087237ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobrescrever a tabela principal com a versão 2\n",
    "# Isso vai gerar uma cópia da versão 2 que será agora a versão principal. \n",
    "old_version.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49a15fca-7299-4d28-ad28-80134f83f270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-----------------------+\n",
      "|id |nome    |idade|funcao                 |\n",
      "+---+--------+-----+-----------------------+\n",
      "|1  |Lucas   |32   |Gerente de Data Science|\n",
      "|3  |Mariana |35   |Arquiteto de Dados     |\n",
      "|4  |Fernando|40   |Engenheiro de Dados    |\n",
      "|6  |Camila  |50   |Engenheiro de ML       |\n",
      "|8  |Juliano |43   |Arquiteto de Dados     |\n",
      "|9  |Gustavo |56   |Arquiteto de Dados     |\n",
      "|10 |Vanessa |31   |Analista de Dados      |\n",
      "+---+--------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar os dados sobrescritos\n",
    "spark.read.format(\"delta\").load(delta_table_path).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd4aaa37-56bb-4249-b96b-7175e263e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tabela tem 6 versões.\n"
     ]
    }
   ],
   "source": [
    "# Caminho para a tabela Delta\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Obter o histórico completo\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# Contar o número de versões\n",
    "num_versions = history_df.count()\n",
    "print(f\"A tabela tem {num_versions} versões.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61a29a-fa2c-4ba7-bc5b-0b4dda4f5880",
   "metadata": {},
   "source": [
    "Considerações:\n",
    "\n",
    "- A operação de sobrescrita cria uma nova versão na tabela Delta. Dados atuais ainda estarão no histórico, mas os dados sobrescritos substituem a visão principal da tabela.\n",
    "\n",
    "- Certifique-se de que o esquema da versão antiga é compatível com o esquema atual. Caso contrário, pode ser necessário habilitar a opção overwriteSchema.\n",
    "\n",
    "- Em ambientes críticos, prefira corrigir os dados com operações como MERGE ou UPDATE em vez de sobrescrever diretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33b762-b09c-4b1b-aaf6-acf3a705e2e3",
   "metadata": {},
   "source": [
    "## Aplicando o VACUUM\n",
    "\n",
    "Por padrão, o Delta Lake impõe uma retenção mínima de 7 dias para garantir que operações como time travel ainda sejam possíveis e para evitar exclusão acidental de dados necessários para transações. Se você quiser reduzir esse período, será necessário modificar a configuração de retenção mínima.\n",
    "\n",
    "Você não poderá acessar versões anteriores além do período de retenção configurado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ba83fa4-ac12-4eea-bc84-8da4cec3534a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desativar temporariamente a proteção para retenção mínima\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005196a-3dd8-4275-921e-d61e0de36a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar VACUUM com retenção de 1 dia\n",
    "print(\"Executando vacuum com retenção de 5 minutos...\")\n",
    "delta_table.vacuum(retentionHours=0.0833) # 5 minutos = 0.0833 horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f638da8c-7e84-4816-ba1a-dd80cd6c1bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reativar a proteção para retenção mínima\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f41803-3f59-4028-b5e1-8aa180b7eb9c",
   "metadata": {},
   "source": [
    "Se o VACUUM foi executado com um período curto de retenção, versões mais antigas podem ter sido excluídas e não estarão disponíveis no histórico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73bba602-145e-4ecb-950a-7560d15de1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tabela tem 6 versões.\n"
     ]
    }
   ],
   "source": [
    "# Caminho para a tabela Delta\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Obter o histórico completo\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# Contar o número de versões\n",
    "num_versions = history_df.count()\n",
    "print(f\"A tabela tem {num_versions} versões.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e354a6a7-372f-414e-95b5-32b5ba55eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar a versão 2\n",
    "old_version = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93178028-26f2-4b5e-a1f8-6b6bb803dc7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o308.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 267.0 failed 4 times, most recent failure: Lost task 0.3 in stage 267.0 (TID 12230) (172.18.0.12 executor 1): org.apache.spark.SparkFileNotFoundException: No such file or directory: s3a://production/landing/dsa/lakehouse/dados_iniciais/part-00000-0492ee30-7f4a-418f-9d38-d488359ba69e-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkFileNotFoundException: No such file or directory: s3a://production/landing/dsa/lakehouse/dados_iniciais/part-00000-0492ee30-7f4a-418f-9d38-d488359ba69e-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mold_version\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o308.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 267.0 failed 4 times, most recent failure: Lost task 0.3 in stage 267.0 (TID 12230) (172.18.0.12 executor 1): org.apache.spark.SparkFileNotFoundException: No such file or directory: s3a://production/landing/dsa/lakehouse/dados_iniciais/part-00000-0492ee30-7f4a-418f-9d38-d488359ba69e-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkFileNotFoundException: No such file or directory: s3a://production/landing/dsa/lakehouse/dados_iniciais/part-00000-0492ee30-7f4a-418f-9d38-d488359ba69e-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "old_version.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "173f542a-c27b-4b77-af85-6d9b63d21034",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47563b4e-239a-491b-9a54-d2fa345cea29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
