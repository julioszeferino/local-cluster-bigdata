apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: ScheduledSparkApplication
metadata:
  name: sch-etl-yelp-batch
  namespace: processing
spec:
  schedule: "@every 5m"
  concurrencyPolicy: Allow
  template:
    type: Python
    mode: cluster
    image: "owshq/etl-yelp-batch:latest"
    imagePullPolicy: Always
    mainApplicationFile: local:///app/etl-yelp-batch.py
    sparkVersion: "3.5.0"
    restartPolicy:
      type: OnFailure
      onFailureRetries: 3
      onFailureRetryInterval: 10
      onSubmissionFailureRetries: 5
      onSubmissionFailureRetryInterval: 20
    sparkConf:
      spark.hadoop.fs.s3a.endpoint: "http://minio.deepstorage.svc.Cluster.local"
      spark.hadoop.fs.s3a.access.key: "data-lake"
      spark.hadoop.fs.s3a.secret.key: "12620ee6-2162-11ee-be56-0242ac120002"
      spark.hadoop.fs.s3a.path.style.access: "True"
      spark.hadoop.fs.s3a.fast.upload: "True"
      spark.hadoop.fs.s3a.multipart.size: "104857600"
      spark.hadoop.fs.s3a.connection.maximum: "100"
      spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
      spark.sql.extensions: "io.delta.sql.DeltaSparkSessionExtension"
      spark.sql.catalog.spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"
      spark.hadoop.fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    driver:
      # cores: 1
      coreRequest: "1"
      coreLimit: "1200m"
      memory: "512m"
      labels:
        version: 3.5.0
      serviceAccount: default
    executor:
      # cores: 1
      coreRequest: "1"
      instances: 2
      memory: "512m"
      labels:
        version: 3.5.0
